# GPT Model Configuration
model:
  vocab_size: 50257
  max_context_length: 256  # Shortened context length (orig: 1024)
  n_embd: 768             # Embedding dimension
  n_head: 12             # Number of attention heads
  n_blocks: 12            # Number of layers
  drop_rate: 0.1          # Dropout rate
  qkv_bias: false         # Query-key-value bias
  n_exp: 2                # Number of experts
  top_k: 2                # Top k experts
  use_noisy_top_k: true   # Use noisy top k
  capacity_factor: 1.25   # Capacity factor
  bias: false             # Bias
  dropout: 0.2            # Dropout rate
  flash_self_attention: false # for faster self-attention
  use_switch_tfm_init: true # Use switch transformer-style initialization
  switch_tfm_init_scale: 1.0 # Switch transformer-style initialization scale
  use_switch_tfm_init_experts: true # Use switch transformer-style initialization for experts
  switch_tfm_init_scale_experts: 1.0 # Switch transformer-style initialization scale for experts
  stride: 2 # One in every stride layers are converted to an MoE
  use_moe: true # Use MoE
  use_aux_loss: true # Use auxiliary loss
  use_router_z_loss: true # Use router z loss
  router_use_full_prec: false # Use full precision in the router
  train_capacity: 1.25 # Train capacity
  eval_capacity: 2.0 # Eval capacity
  min_capacity: 4 # Minimum capacity
  aux_loss_weight: 0.001 # Auxiliary loss weight
  router_z_loss_weight: 0.01 # Router z loss weight

# Training Settings
training:
  learning_rate: 0.0005  # 5e-4
  num_epochs: 10
  batch_size: 2
  weight_decay: 0.1
  seed: 123
  max_iters: 300  # Maximum training iterations
  eval_interval: 50  # Evaluate every N iterations
  log_interval: 10  # Log every N iterations
  eval_iters: 40  # Number of evaluation iterations
  grad_clip: 1.0  # Gradient clipping value
  eval_only: false # Only evaluate the model

# Learning Rate Schedule Settings
lr_schedule:
  decay_lr: true  # Enable learning rate decay
  warmup_iters: 1000  # Warmup iterations
  lr_decay_iters: 10000  # Learning rate decay iterations
  min_lr: 0.0001  # Minimum learning rate (1e-4)

optimizer:
  weight_decay: 0.1
  learning_rate: 0.0005
  betas: [0.9, 0.95]
  device_type: "cpu"

# Data Settings
data:
  dataset: "data/shakespeare"
  file_path: "gpt_moe/data/train_data/shakespeare.txt"
  train_ratio: 0.90
  eval_freq: 5
  eval_iter: 1

# Output Settings
output:
  model_save_path: "gpt_model.pth"
  loss_plot_path: "loss.pdf"
  always_save_checkpoint: true
  checkpoint_path: "gpt_moe/checkpoints"
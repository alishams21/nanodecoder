# GPT Model Configuration
model:
  vocab_size: 50257
  max_context_length: 256  # Shortened context length (orig: 1024)
  n_embd: 768             # Embedding dimension
  n_head: 12             # Number of attention heads
  n_blocks: 12            # Number of layers
  drop_rate: 0.1          # Dropout rate
  qkv_bias: false         # Query-key-value bias
  n_exp: 2                # Number of experts
  top_k: 1                # Top k experts
  use_noisy_top_k: true   # Use noisy top k
  capacity_factor: 1.25   # Capacity factor
  bias: false             # Bias
  dropout: 0.2            # Dropout rate
  flash_self_attention: false # for faster self-attention
  use_switch_tfm_init: true # Use switch transformer-style initialization
  switch_tfm_init_scale: 1.0 # Switch transformer-style initialization scale
  use_switch_tfm_init_experts: true # Use switch transformer-style initialization for experts
  switch_tfm_init_scale_experts: 1.0 # Switch transformer-style initialization scale for experts
  stride: 2 # One in every stride layers are converted to an MoE
  use_moe: true # Use MoE
  use_aux_loss: true # Use auxiliary loss
  use_router_z_loss: true # Use router z loss
  router_use_full_prec: false # Use full precision in the router
  train_capacity: 1.25 # Train capacity
  eval_capacity: 2.0 # Eval capacity
  min_capacity: 4 # Minimum capacity
  aux_loss_weight: 0.001 # Auxiliary loss weight
  router_z_loss_weight: 0.01 # Router z loss weight

device:
  device_type: "cuda"
  num_workers: 0
  pin_memory: true
  non_blocking: true
  tf32: true
  cudnn_tf32: true
  cudnn_benchmark: false
  cudnn_deterministic: false
  cudnn_allow_tf32: true
  cudnn_allow_tf32_autotune: true
  cudnn_allow_tf32_autotune_deterministic: false
  cudnn_allow_tf32_autotune_deterministic_deterministic: false
  nproc_per_node: 1  # Number of processes per node for DDP training (use with torchrun --nproc_per_node=N)

# Model Compilation Settings (PyTorch 2.0+)
compile:
  enabled: true  # Enable model compilation for additional speedup
  mode: "default"  # Compilation mode: "default", "reduce-overhead", "max-autotune"
  fullgraph: false  # Whether to compile the full graph
  dynamic: null  # Whether to use dynamic shapes

# Memory Management Settings
memory:
  enable_memory_pool: true     # Enable memory pooling for tensor reuse
  buffer_size: 2              # Number of batches to prefetch (2-4 recommended)
  max_pool_size: 4            # Maximum number of tensors to keep in memory pool
  enable_prefetching: true     # Enable background batch prefetching
  prefetch_timeout: 5.0        # Timeout for prefetch operations (seconds)
  memory_monitoring: true     # Enable memory usage monitoring
  auto_optimize: true         # Automatically optimize settings based on hardware

# Training Settings
training:
  learning_rate: 0.0005  # 5e-4
  num_epochs: 10
  batch_size: 8
  weight_decay: 0.1
  seed: 123
  max_iters: 1000  # Maximum training iterations
  eval_interval: 100  # Evaluate every N iterations
  log_interval: 20  # Log every N iterations
  eval_iters: 10  # Number of batches per evaluation cycle
  grad_clip: 1.0  # Gradient clipping value
  eval_only: false # Only evaluate the model
  best_val_loss: 1000 # Best validation loss this is for initialisation
  accumulation_steps: 4  # number of gradient accumulation steps each individual GPU performs (1 = no accumulation)

# Learning Rate Schedule Settings
lr_schedule:
  decay_lr: true  # Enable learning rate decay
  warmup_iters: 400  # Warmup iterations
  lr_decay_iters: 700  # Learning rate decay iterations
  min_lr: 0.0001  # Minimum learning rate (1e-4)

optimizer:
  weight_decay: 0.1
  learning_rate: 0.0005
  betas: [0.9, 0.95]

# Data Settings
data:
  dataset: "data/shakespeare"
  train_ratio: 0.90
  eval_freq: 5
  eval_iter: 1

# Output Settings
output:
  model_save_path: "gpt_model.pth"
  loss_plot_path: "loss.pdf"
  always_save_checkpoint: true
  checkpoint_path: "gpt_moe/checkpoints"

# Wandb Logging Settings
wandb:
  enabled: true
  project: "nanodecoder-distributed-run"
  run_name: "gpt-moe-distributed"
  log_interval: 1